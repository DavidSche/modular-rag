{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypster import HP, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def indexing_config(hp: HP):\n",
    "    from haystack.components.converters import PyPDFToDocument\n",
    "\n",
    "    converter = PyPDFToDocument()\n",
    "\n",
    "    from haystack import Pipeline\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"loader\", converter)\n",
    "\n",
    "    enrich_doc_w_llm = hp.select([True, False], default=True)\n",
    "    if enrich_doc_w_llm:\n",
    "        from haystack.components.builders import PromptBuilder\n",
    "\n",
    "        from src.haystack_utils import AddLLMMetadata\n",
    "\n",
    "        template = hp.text_input(\"\"\"\n",
    "        Please provide a one sentence summary of 15 words max what this document is about.\n",
    "        Then add a list of 3-5 keywords, including acronyms, that will help to find this document using keywords.\n",
    "        Context:\n",
    "        {{ documents[0].content[:1000] }}\n",
    "        Output format:\n",
    "\n",
    "        Summary:\n",
    "        Keywords:\n",
    "        \"\"\")\n",
    "        from hypster import load\n",
    "\n",
    "        llm = load(\"configs/llm.py\")\n",
    "        llm_inputs = hp.propagate(llm)\n",
    "        pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "        pipeline.add_component(\"llm\", llm_inputs[\"llm\"])\n",
    "        pipeline.add_component(\"document_enricher\", AddLLMMetadata())\n",
    "        pipeline.connect(\"loader\", \"prompt_builder\")\n",
    "        pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "        pipeline.connect(\"llm\", \"document_enricher\")\n",
    "        pipeline.connect(\"loader\", \"document_enricher\")\n",
    "\n",
    "        doc_source = \"document_enricher\"\n",
    "    else:\n",
    "        doc_source = \"loader\"\n",
    "\n",
    "    from haystack.components.preprocessors import DocumentSplitter\n",
    "\n",
    "    split_by = hp.select([\"sentence\", \"word\", \"passage\", \"page\"], default=\"sentence\")\n",
    "    splitter = DocumentSplitter(split_by=split_by, split_length=hp.int_input(10), split_overlap=hp.int_input(2))\n",
    "\n",
    "    pipeline.add_component(\"splitter\", splitter)\n",
    "    pipeline.connect(doc_source, \"splitter\")\n",
    "\n",
    "\n",
    "indexing_config.save(\"configs/indexing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def fast_embed(hp: HP):\n",
    "    from typing import Any, Dict, List\n",
    "\n",
    "    from fastembed import TextEmbedding\n",
    "\n",
    "    def get_model_dim(chosen_model: str, model_list: List[Dict[str, Any]]) -> int:\n",
    "        for model in model_list:\n",
    "            if model[\"model\"] == chosen_model:\n",
    "                return model[\"dim\"]\n",
    "        raise ValueError(f\"Model {chosen_model} not found in the list of supported models.\")\n",
    "\n",
    "    from haystack_integrations.components.embedders.fastembed import (\n",
    "        FastembedDocumentEmbedder,\n",
    "        FastembedTextEmbedder,\n",
    "    )\n",
    "\n",
    "    meta_fileds_to_embed = [\"parent_doc_summary\"]\n",
    "\n",
    "    model = hp.select(\n",
    "        {\"bge-small\": \"BAAI/bge-small-en-v1.5\", \"mini-lm\": \"sentence-transformers/all-MiniLM-L6-v2\"},\n",
    "        default=\"mini-lm\",\n",
    "    )\n",
    "    import os\n",
    "\n",
    "    cpu_count = os.cpu_count() or 1\n",
    "    doc_embedder = FastembedDocumentEmbedder(\n",
    "        model=model,\n",
    "        parallel=hp.int_input(cpu_count),\n",
    "        meta_fields_to_embed=meta_fileds_to_embed,\n",
    "    )\n",
    "    text_embedder = FastembedTextEmbedder(model=model)\n",
    "    embedding_dim = get_model_dim(model, TextEmbedding.list_supported_models())\n",
    "\n",
    "\n",
    "fast_embed.save(\"configs/fast_embed.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def jina_embed(hp: HP):\n",
    "    from haystack_integrations.components.embedders.jina import JinaDocumentEmbedder, JinaTextEmbedder\n",
    "\n",
    "    meta_fileds_to_embed = [\"parent_doc_summary\"]\n",
    "\n",
    "    model = hp.select({\"v3\": \"jina-embeddings-v3\", \"v2\": \"jina-embeddings-v2\"}, default=\"v3\")\n",
    "    late_chunking = hp.select([True, False], default=True, name=\"late_chunking\") if model == \"v3\" else False\n",
    "    doc_embedder = JinaDocumentEmbedder(\n",
    "        model=model,\n",
    "        batch_size=hp.int_input(16),\n",
    "        dimensions=hp.int_input(256),\n",
    "        task=\"retrieval.passage\",\n",
    "        late_chunking=late_chunking,\n",
    "        meta_fields_to_embed=meta_fileds_to_embed,\n",
    "    )\n",
    "    text_embedder = JinaTextEmbedder(model=model, dimensions=doc_embedder.dimensions, task=\"retrieval.query\")\n",
    "    embedding_dim = doc_embedder.dimensions\n",
    "\n",
    "\n",
    "jina_embed.save(\"configs/jina_embed.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def in_memory_retrieval(hp: HP):\n",
    "    from haystack.components.retrievers.in_memory import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "    from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "    embedding_similarity_function = hp.select([\"cosine\", \"dot_product\"], default=\"cosine\")\n",
    "    bm25_algorithm = hp.select([\"BM25Okapi\", \"BM25L\", \"BM25Plus\"], default=\"BM25L\")\n",
    "    document_store = InMemoryDocumentStore(\n",
    "        embedding_similarity_function=embedding_similarity_function, bm25_algorithm=bm25_algorithm\n",
    "    )\n",
    "\n",
    "    from haystack.components.joiners.document_joiner import DocumentJoiner\n",
    "\n",
    "    join_mode = hp.select(\n",
    "        [\"distribution_based_rank_fusion\", \"concatenate\", \"merge\", \"reciprocal_rank_fusion\"],\n",
    "        default=\"distribution_based_rank_fusion\",\n",
    "    )\n",
    "    joiner = DocumentJoiner(join_mode=join_mode, top_k=hp.int_input(10))\n",
    "\n",
    "    from haystack import Pipeline\n",
    "\n",
    "    from src.haystack_utils import PassThroughDocuments, PassThroughText\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"query\", PassThroughText())\n",
    "    pipeline.add_component(\"bm25_retriever\", InMemoryBM25Retriever(document_store=document_store))\n",
    "    pipeline.add_component(\"embedding_retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "    pipeline.add_component(\"document_joiner\", joiner)\n",
    "    pipeline.add_component(\"retrieved_documents\", PassThroughDocuments())\n",
    "    pipeline.connect(\"query\", \"bm25_retriever\")\n",
    "    pipeline.connect(\"bm25_retriever\", \"document_joiner\")\n",
    "    pipeline.connect(\"embedding_retriever\", \"document_joiner\")\n",
    "    pipeline.connect(\"document_joiner\", \"retrieved_documents\")\n",
    "\n",
    "\n",
    "in_memory_retrieval.save(\"configs/in_memory_retrieval.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def qdrant_retrieval(hp: HP):\n",
    "    from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "    from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "\n",
    "    location = hp.text_input(\":memory:\")\n",
    "    embedding_similarity_function = hp.select([\"cosine\", \"dot_product\", \"l2\"], default=\"cosine\")\n",
    "\n",
    "    document_store = QdrantDocumentStore(\n",
    "        location=location,\n",
    "        recreate_index=True,\n",
    "        similarity=embedding_similarity_function,\n",
    "        embedding_dim = hp.int_input(256),\n",
    "        on_disk=True,\n",
    "    )\n",
    "\n",
    "    embedding_retriever = QdrantEmbeddingRetriever(document_store=document_store, top_k=hp.int_input(20))\n",
    "\n",
    "    from haystack import Pipeline\n",
    "\n",
    "    from src.haystack_utils import PassThroughDocuments, PassThroughText\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"query\", PassThroughText())\n",
    "    pipeline.add_component(\"embedding_retriever\", embedding_retriever)\n",
    "    pipeline.add_component(\"retrieved_documents\", PassThroughDocuments())\n",
    "    pipeline.connect(\"embedding_retriever\", \"retrieved_documents\")\n",
    "\n",
    "\n",
    "qdrant_retrieval.save(\"configs/qdrant_retrieval.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def llm(hp: HP):\n",
    "    anthropic_models = {\"haiku\": \"claude-3-haiku-20240307\", \"sonnet\": \"claude-3-5-sonnet-20240620\"}\n",
    "    openai_models = {\"gpt-4o-mini\": \"gpt-4o-mini\", \"gpt-4o\": \"gpt-4o\", \"gpt-4o-latest\": \"gpt-4o-2024-08-06\"}\n",
    "    model = hp.select({**anthropic_models, **openai_models}, default=\"gpt-4o-mini\")\n",
    "    from haystack.components.generators import OpenAIGenerator\n",
    "    from haystack_integrations.components.generators.anthropic import AnthropicGenerator\n",
    "\n",
    "    llm = AnthropicGenerator(model=model) if model in anthropic_models.values() else OpenAIGenerator(model=model)\n",
    "\n",
    "\n",
    "llm.save(\"configs/llm.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def reranker(hp: HP):\n",
    "    jina_models = {\n",
    "        \"reranker-v2\": \"jina-reranker-v2-base-multilingual\",\n",
    "        \"colbert-v2\": \"jina-colbert-v2\",\n",
    "        \"reranker-v1\": \"jina-reranker-v1-base-en\",\n",
    "    }\n",
    "\n",
    "    transformers_models = {\n",
    "        \"tiny-bert-v2\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",\n",
    "        \"minilm-v2\": \"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
    "    }\n",
    "\n",
    "    model = hp.select({**jina_models, **transformers_models}, default=\"reranker-v2\")\n",
    "    if model in jina_models.values():\n",
    "        from haystack_integrations.components.rankers.jina import JinaRanker\n",
    "\n",
    "        reranker = JinaRanker(model=model, top_k=hp.int_input(3))\n",
    "    else:\n",
    "        from haystack.components.rankers import TransformersSimilarityRanker\n",
    "\n",
    "        reranker = TransformersSimilarityRanker(model=model, top_k=hp.int_input(3))\n",
    "\n",
    "\n",
    "reranker.save(\"configs/reranker.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def response_config(hp: HP):\n",
    "    from hypster import load\n",
    "    llm_config = load(\"configs/llm.py\")\n",
    "    response_llm = hp.propagate(llm_config)\n",
    "\n",
    "    from textwrap import dedent\n",
    "\n",
    "    from haystack import Pipeline\n",
    "    from haystack.components.builders import PromptBuilder\n",
    "    template = dedent(\"\"\"\n",
    "    Given the following information, answer the question in one short sentence, \n",
    "    using the information provided in the documents. \n",
    "    Add an exact quote from the documents that you based your answer on.\n",
    "    Note: In some cases, only one or some of the documents will be relevant to the question.\n",
    "    ========================================\n",
    "    Context:\n",
    "    {% for document in documents %}\n",
    "        <Document {{ document.id }}>\n",
    "        <LLM Extracted Information>\n",
    "        {{ document.meta.llm_extracted_info }}\n",
    "        <\\LLM Extracted Information>\n",
    "        <Document Content>\n",
    "        {{ document.content }}\n",
    "        <\\Document Content>\n",
    "        <\\Document {{ document.id }}>\n",
    "        \\n\n",
    "    {% endfor %}\n",
    "    ========================================\n",
    "    Question: {{question}}\n",
    "    Chain of Thought:\n",
    "    Answer:\n",
    "    Quotes:\n",
    "    \"\"\")\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "    pipeline.add_component(\"llm\", response_llm[\"llm\"])\n",
    "    pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "response_config.save(\"configs/response.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config\n",
    "def hp_config(hp: HP):\n",
    "    from hypster import load\n",
    "\n",
    "    file_path = hp.text_input(\"data/raw/modular_rag.pdf\")\n",
    "    query = \"What is the use of BERT in this document?\"\n",
    "\n",
    "    indexing_config = load(\"configs/indexing.py\")\n",
    "    indexing = hp.propagate(indexing_config)\n",
    "    indexing_pipeline = indexing[\"pipeline\"]\n",
    "\n",
    "    fast_embed = load(\"configs/fast_embed.py\")\n",
    "    jina_embed = load(\"configs/jina_embed.py\")\n",
    "    embedder = hp.select({\"fastembed\": hp.propagate(fast_embed), \"jina\": hp.propagate(jina_embed)}, default=\"fastembed\")\n",
    "    indexing_pipeline.add_component(\"doc_embedder\", embedder[\"doc_embedder\"])\n",
    "\n",
    "    qdrant = load(\"configs/qdrant_retrieval.py\")\n",
    "    in_memory = load(\"configs/in_memory_retrieval.py\")\n",
    "    document_store = hp.select(\n",
    "        {\"in_memory\": hp.propagate(in_memory), \"qdrant\": hp.propagate(qdrant)}, default=\"in_memory\"\n",
    "    )\n",
    "\n",
    "    from haystack.components.writers import DocumentWriter\n",
    "    from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "    document_writer = DocumentWriter(document_store[\"document_store\"], policy=DuplicatePolicy.OVERWRITE)\n",
    "    indexing_pipeline.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "    indexing_pipeline.connect(\"splitter\", \"doc_embedder\")\n",
    "    indexing_pipeline.connect(\"doc_embedder\", \"document_writer\")\n",
    "\n",
    "    retrieval_pipeline = document_store[\"pipeline\"]\n",
    "    retrieval_pipeline.add_component(\"text_embedder\", embedder[\"text_embedder\"])\n",
    "    retrieval_pipeline.connect(\"query\", \"text_embedder\")\n",
    "    retrieval_pipeline.connect(\"text_embedder\", \"embedding_retriever\")\n",
    "\n",
    "    from src.haystack_utils import PassThroughDocuments\n",
    "\n",
    "    retrieval_pipeline.add_component(\"docs_for_generation\", PassThroughDocuments())\n",
    "    use_reranker = hp.select([True, False], default=True)\n",
    "    if use_reranker:\n",
    "        reranker_config = load(\"configs/reranker.py\")\n",
    "        reranker = hp.propagate(reranker_config)\n",
    "        retrieval_pipeline.add_component(\"reranker\", reranker[\"reranker\"])\n",
    "        retrieval_pipeline.connect(\"retrieved_documents\", \"reranker\")\n",
    "        retrieval_pipeline.connect(\"reranker\", \"docs_for_generation\")\n",
    "        retrieval_pipeline.connect(\"query\", \"reranker\")\n",
    "    else:\n",
    "        retrieval_pipeline.connect(\"retrieved_documents\", \"docs_for_generation\")\n",
    "\n",
    "    response_config = load(\"configs/response.py\")\n",
    "    response = hp.propagate(response_config)\n",
    "    response_pipeline = response[\"pipeline\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = hp_config(\n",
    "    selections={\n",
    "        \"large_llm_model.model\": \"gpt-4o\",\n",
    "        \"document_store\": \"in_memory\",\n",
    "        \"document_store.bm25_algorithm\": \"BM25Plus\",\n",
    "        \"indexing_inputs.llm_inputs.model\": \"gpt-4o-mini\",\n",
    "        \"reranker.model\": \"tiny-bert-v2\",\n",
    "    },\n",
    "    overrides={\"indexing.splitter.split_length\" : 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().update(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 92385.55it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 60963.72it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 8090.86it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 106997.55it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 85948.85it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 61680.94it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 8185.60it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 9646.51it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 19152.07it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 7741.42it/s]\n",
      "Calculating embeddings: 100%|██████████| 623/623 [00:15<00:00, 39.31it/s]\n",
      "Calculating embeddings: 100%|██████████| 221/221 [00:02<00:00, 84.95it/s]\n"
     ]
    }
   ],
   "source": [
    "indexing_pipeline.warm_up()\n",
    "file_paths = [\"data/raw/modular_rag.pdf\", \"data/raw/enhancing_rag.pdf\"]\n",
    "for file_path in file_paths:\n",
    "    indexing_pipeline.run({\"loader\": {\"sources\": [file_path]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieval_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_pipeline.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00, 229.00it/s]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the use of BERT or ColBERT in RAG?\"\n",
    "retrieval = retrieval_pipeline.run({\"query\": {\"text\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The use of tensor core will help in optimization of inference speed in the \n",
      "encoding process in the RAG pipeline.  \n",
      " \n",
      "In this upcoming section, the approach taken to survey optimization techniques  for RAG systems is outlined.  \n",
      " \n",
      "Table  1: Literature  survey  summary  table  \n",
      " \n",
      "Study  Focus  Approach  Key  Findings  \n",
      " \n",
      "Pierre  et \n",
      "al. Query  \n",
      "optimization  in \n",
      "RAG  BERT,  Orca2,  \n",
      "prompt augmenter  Improved  accuracy,  \n",
      "rel- \n",
      "evance,  and \n",
      "contextual  richness   \n",
      "in  document  \n",
      "retrieval  \n",
      "Gao et al.\n",
      "{'file_path': 'data/raw/enhancing_rag.pdf', 'llm_extracted_info': 'Summary:  \\nThis document is an issue of the International Journal of Scientific Research in Engineering and Management.\\n\\nKeywords:  \\nIJSREM, scientific research, engineering, management, SJIF', 'source_id': '2ca957eaedc6549878e65411ada94d966ac0fa4edc9f9b10d2577c10c8dcc11e', 'page_number': 4, 'split_id': 50, 'split_idx_start': 13456, '_split_overlap': [{'doc_id': '610eb77156aa51862cc3c87bf65da517e92e2fc4c7f921b12c63b7cb848748c7', 'range': (66, 293)}, {'doc_id': '6703bfc8beae40c67e3853702e6a859631e714941ec09ce8ccdd313fc08918f5', 'range': (0, 280)}]}\n",
      "\n",
      " Typical models\n",
      "include BERT structure PLMs, like ColBERT, and multi-task\n",
      "fine-tuned models like BGE [40] and GTE [41].\n",
      "Hybrid Retriever is to use both sparse and dense retrievers\n",
      "simultaneously. Two embedding techniques complement each\n",
      "other to enhance retrieval effectiveness. Sparse retriever can\n",
      "provide initial screening results.\n",
      "{'file_path': 'data/raw/modular_rag.pdf', 'llm_extracted_info': '**Summary:**  \\nThis document discusses the evolution of Retrieval-augmented Generation (RAG) into modular frameworks.  \\n\\n**Keywords:**  \\nRAG, LLM, Modular Frameworks, Knowledge-Intensive Tasks, Advanced Retrievers', 'source_id': '5f28e9ba2f68a429a7e1eb20312eca74fa1e77a4aabed058a078104a878815f9', 'page_number': 6, 'split_id': 128, 'split_idx_start': 26524, '_split_overlap': [{'doc_id': 'd0a883b576226fa86dcbe79145660807e1d78986fd14b14711e036b0d8e9db42', 'range': (217, 412)}, {'doc_id': '562096caafbf59f2c6713392d373a6ae0573649a91dfdb0505aa3d7e673b8145', 'range': (0, 139)}]}\n",
      "\n",
      "  \n",
      " \n",
      "Table  1: Literature  survey  summary  table  \n",
      " \n",
      "Study  Focus  Approach  Key  Findings  \n",
      " \n",
      "Pierre  et \n",
      "al. Query  \n",
      "optimization  in \n",
      "RAG  BERT,  Orca2,  \n",
      "prompt augmenter  Improved  accuracy,  \n",
      "rel- \n",
      "evance,  and \n",
      "contextual  richness   \n",
      "in  document  \n",
      "retrieval  \n",
      "Gao et al. Evolution  and \n",
      "chal- lenges  in \n",
      "RAG  systems  Survey  of RAG  \n",
      "tech- niques  Critical   \n",
      "evaluation   of \n",
      "RAG   challenges   \n",
      "and \n",
      "optimization  \n",
      "avenues  \n",
      " \n",
      "Jiang  et \n",
      "al. Enhancing  RAG  \n",
      "with dynamic \n",
      "retrieval   \n",
      "FLARE  method  Improved\n",
      " perfor\n",
      "- \n",
      "mance  in long-\n",
      "form  knowledge -\n",
      "intensive  \n",
      "tasks  \n",
      " \n",
      "Jin et al.\n",
      "{'file_path': 'data/raw/enhancing_rag.pdf', 'llm_extracted_info': 'Summary:  \\nThis document is an issue of the International Journal of Scientific Research in Engineering and Management.\\n\\nKeywords:  \\nIJSREM, scientific research, engineering, management, SJIF', 'source_id': '2ca957eaedc6549878e65411ada94d966ac0fa4edc9f9b10d2577c10c8dcc11e', 'page_number': 4, 'split_id': 51, 'split_idx_start': 13683, '_split_overlap': [{'doc_id': '73ee2e2811f8605325948a50f94593eb365470a5eb4c4db6f7ab136ff26d88a0', 'range': (227, 507)}, {'doc_id': '005c00dd3dc243e86a57ddd70b66842299fb7eace22e722599d384c9ec561e45', 'range': (0, 321)}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in retrieval[\"docs_for_generation\"][\"documents\"]:\n",
    "    print(doc.content)\n",
    "    print(doc.meta)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=73ee2e2811f8605325948a50f94593eb365470a5eb4c4db6f7ab136ff26d88a0, content: ' The use of tensor core will help in optimization of inference speed in the \n",
       " encoding process in the...', meta: {'file_path': 'data/raw/enhancing_rag.pdf', 'llm_extracted_info': 'Summary:  \\nThis document is an issue of the International Journal of Scientific Research in Engineering and Management.\\n\\nKeywords:  \\nIJSREM, scientific research, engineering, management, SJIF', 'source_id': '2ca957eaedc6549878e65411ada94d966ac0fa4edc9f9b10d2577c10c8dcc11e', 'page_number': 4, 'split_id': 50, 'split_idx_start': 13456, '_split_overlap': [{'doc_id': '610eb77156aa51862cc3c87bf65da517e92e2fc4c7f921b12c63b7cb848748c7', 'range': (66, 293)}, {'doc_id': '6703bfc8beae40c67e3853702e6a859631e714941ec09ce8ccdd313fc08918f5', 'range': (0, 280)}]}, score: 0.9821396470069885, embedding: vector of size 384),\n",
       " Document(id=d7f6628affb0ad7d3ba2f89658508f3728d3fb749782fd5c2d005bf9ecc726f8, content: ' Typical models\n",
       " include BERT structure PLMs, like ColBERT, and multi-task\n",
       " fine-tuned models like BGE...', meta: {'file_path': 'data/raw/modular_rag.pdf', 'llm_extracted_info': '**Summary:**  \\nThis document discusses the evolution of Retrieval-augmented Generation (RAG) into modular frameworks.  \\n\\n**Keywords:**  \\nRAG, LLM, Modular Frameworks, Knowledge-Intensive Tasks, Advanced Retrievers', 'source_id': '5f28e9ba2f68a429a7e1eb20312eca74fa1e77a4aabed058a078104a878815f9', 'page_number': 6, 'split_id': 128, 'split_idx_start': 26524, '_split_overlap': [{'doc_id': 'd0a883b576226fa86dcbe79145660807e1d78986fd14b14711e036b0d8e9db42', 'range': (217, 412)}, {'doc_id': '562096caafbf59f2c6713392d373a6ae0573649a91dfdb0505aa3d7e673b8145', 'range': (0, 139)}]}, score: 0.9104085564613342, embedding: vector of size 384),\n",
       " Document(id=6703bfc8beae40c67e3853702e6a859631e714941ec09ce8ccdd313fc08918f5, content: '  \n",
       "  \n",
       " Table  1: Literature  survey  summary  table  \n",
       "  \n",
       " Study  Focus  Approach  Key  Findings  \n",
       "  \n",
       " Pier...', meta: {'file_path': 'data/raw/enhancing_rag.pdf', 'llm_extracted_info': 'Summary:  \\nThis document is an issue of the International Journal of Scientific Research in Engineering and Management.\\n\\nKeywords:  \\nIJSREM, scientific research, engineering, management, SJIF', 'source_id': '2ca957eaedc6549878e65411ada94d966ac0fa4edc9f9b10d2577c10c8dcc11e', 'page_number': 4, 'split_id': 51, 'split_idx_start': 13683, '_split_overlap': [{'doc_id': '73ee2e2811f8605325948a50f94593eb365470a5eb4c4db6f7ab136ff26d88a0', 'range': (227, 507)}, {'doc_id': '005c00dd3dc243e86a57ddd70b66842299fb7eace22e722599d384c9ec561e45', 'range': (0, 321)}]}, score: 0.14281722903251648)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval[\"docs_for_generation\"][\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = inputs[\"response_pipeline\"]\n",
    "pipeline.warm_up()\n",
    "response = pipeline.run(\n",
    "    {\"prompt_builder\": {\"question\": query, \"documents\": retrieval[\"docs_for_generation\"][\"documents\"]}},\n",
    "    include_outputs_from={\"prompt_builder\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT or ColBERT is used in RAG to enhance retrieval effectiveness through hybrid approaches that combine sparse and dense retrievers. \n",
      "\n",
      "Quote: \"Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41].\"\n"
     ]
    }
   ],
   "source": [
    "print(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt-4o-mini-2024-07-18',\n",
       "  'index': 0,\n",
       "  'finish_reason': 'stop',\n",
       "  'usage': {'completion_tokens': 64,\n",
       "   'prompt_tokens': 943,\n",
       "   'total_tokens': 1007,\n",
       "   'completion_tokens_details': CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0),\n",
       "   'prompt_tokens_details': PromptTokensDetails(audio_tokens=None, cached_tokens=0)}}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"llm\"][\"meta\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
